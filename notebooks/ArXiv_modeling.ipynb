{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Recommendation of similar articles from journal abstract analysis'  \n",
    "# Modeling for recommendation creation\n",
    "## 2019, Misty M. Giles\n",
    "### https://github.com/OhThatMisty/astro_categories/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_short\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import spacy\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    '''Convert to ascii, remove special characters associated with LaTeX when given a df column,\n",
    "       only keep alpha chars and contractions/posessives'''\n",
    "    normalized_text = []\n",
    "    \n",
    "    for t in text:\n",
    "        t = unicodedata.normalize('NFKD', t).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        # This line is necessary to separate some words since the LaTeX/math formatting is getting\n",
    "        # quite a lot of attention in the w2v section.  \n",
    "        t = re.sub('mathrm', ' ', t) \n",
    "        # Expand \"not\" before removing punctuation \n",
    "        t = re.sub('n\\'t', ' not', t)\n",
    "        t = strip_punctuation(t)\n",
    "        # This line gets rid of non-alpha (mostly for digits now) \n",
    "        t = re.sub('[^A-Za-z]+', ' ', t) \n",
    "        normalized_text.append(strip_short(t))\n",
    "    # strip_short gets rid of the rest of the math leftovers (and some abbreviations, like ir for\n",
    "    # infrared - judged that the random letters left over from the math and units of measurement like\n",
    "    # km, mm caused more noise than losing a few v short acronyms would cause problems - issue seen \n",
    "    # more frequently in tfidf)\n",
    "    return normalized_text\n",
    "\n",
    "# This function is to remove excess whitespace \n",
    "def remove(token):\n",
    "    '''Provide feedback on whether a token is excess whitespace'''\n",
    "    return token.is_space\n",
    "\n",
    "# This function ensures that all printouts use the same formula\n",
    "def join_tokens(sent):\n",
    "    '''Joins tokens in a sent without whatever is in remove(), adds pronoun back\n",
    "       in instead of -PRON-'''\n",
    "    return ' '.join([token.lemma_ if token.lemma_ != '-PRON-' else token.text.lower()\n",
    "                     for token in sent if not remove(token)])\n",
    "\n",
    "# This function prevents nested lists that kill the vectorizer\n",
    "def join_sentences(doc):\n",
    "    '''Joins sentences in a doc (includes join_tokens)'''\n",
    "    return ' '.join([join_tokens(sent) for sent in doc.sents])\n",
    "\n",
    "# Set up spacy to lemmatize the text\n",
    "nlp = spacy.load('en', disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, int)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the csv file created in the cleaning notebook\n",
    "file = os.path.join('..', 'data', 'astro_intermediate.csv')\n",
    "df = pd.read_csv(file, index_col=0)\n",
    "\n",
    "# Set variables for testing speed\n",
    "docs_to_run = 1000  # 1000 for testing, len(df) for real processing\n",
    "train_docs = 800 if docs_to_run == 1000 else 50000\n",
    "test_docs = docs_to_run - train_docs\n",
    "train_docs, type(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Don't remove stopwords at this point.  Do that at the modeling stage with the models' \n",
    "# built-ins.  Screws up the phrasing.\n",
    "\n",
    "# Clean the file (remove punctuation, lowercase, lemmatize, remove 1- and 2-char objects --\n",
    "# most are math/LaTeX formatting leftovers or possessives)\n",
    "text = [join_sentences(doc) for doc in nlp.pipe(normalize(df.abstract[:docs_to_run]), batch_size=1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['high resolution alma ghz and vla ghz measurement have be use image continuum and spectral line emission from the inner region the nearby infrared luminous galaxy detect compact luminous continuum emission the core with brightness temperature the ghz continuum equally compact but fainter flux suggest that the continuum opaque wavelength imply very large column density and that emerge from hot dust with temperature sim vibrationally excited line hcn and hcn vib be see emission and resolve scale the hcn vib emission reveal north south nuclear velocity gradient with projected rotation velocity kms the bright hcn vib emission orient perpendicular the velocity gradient ground state line hcn hco and show complex line absorption and emission feature hcn and hco have red shift reversed cygni profile consistent with gas inflow sim km the absorption feature can traced from the north east into the nucleus contrast show blue shift line wing extend km suggest that dense and slow outflow hide behind foreground layer inflow gas appear that the centre phase rapid evolution where inflow build the nuclear column density gas slow dense outflow may signal the onset feedback the inner luminosity can power accreting black hole and compact starburst with top heavy initial mass function']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[22:23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Now that the text has been prepared, it's time to choose a sample abstract from the set of abstracts that won't be used to train the models.  This is a proof-of-concept method for testing the recommendation engine that can be tested even with internet issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General relativistic effects in the galaxy bias at second order \n",
      "\n",
      "The local galaxy bias formalism relies on the energy constraint equation at the formation time to relate the metric perturbation to the matter density contrast. In the Newtonian approximation, this relationship is linear, which allows us to specify the initial galaxy density as a function of local physical operators. In general relativity however, the relationship is intrinsically nonlinear and a modulation of the short-wavelength mode by the long-wavelength mode might be expected. We describe in detail how to obtain local coordinates where the coupling of the long- to the short-wavelength modes is removed through a change of coordinates (in the absence of the primordial non-Gaussianity). We derive the general-relativistic correction to the galaxy bias expansion at second order. The correction does not come from the modulation of small-scale clustering by the long-wavelength mode; instead, it arises from distortions of the volume element by the long-wavelength mode and it does not lead to new bias parameters. \n",
      "\n",
      "astro-ph.CO|gr-qc\n"
     ]
    }
   ],
   "source": [
    "# Pick an article to function as the sample and print some attributes.  \n",
    "# Abstract is unaltered from download (more human-readable but includes formatting).\n",
    "article_idx = np.random.randint(0, test_docs)\n",
    "print(df.title.iloc[(article_idx + train_docs)], '\\n')\n",
    "print(df.abstract.iloc[(article_idx + train_docs)], '\\n')\n",
    "print(df.terms.iloc[(article_idx + train_docs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model: tfidf using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set up the model for vectorizing/calculating the similarity; words must appear \n",
    "# in at least 500 documents.  sklearn stopwords are used, as removing stopwords at\n",
    "# the beginning proved to damage the ngram results.\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3), min_df=0.01, stop_words='english')\n",
    "\n",
    "# Transform/fit the training and test data to the model\n",
    "tfidf_matrix = tfidf.fit_transform(text[:train_docs]).todense()\n",
    "article_matrix = tfidf.transform(text[train_docs:]).todense()\n",
    "\n",
    "# Create a df of the model's values\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix, columns=tfidf.get_feature_names())\n",
    "article_df = pd.DataFrame(article_matrix, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1836"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "continuum         0.274706\n",
       "inflow            0.269927\n",
       "emission          0.262478\n",
       "line              0.227307\n",
       "ghz               0.215898\n",
       "compact           0.179151\n",
       "north             0.170520\n",
       "gradient          0.159100\n",
       "column density    0.153772\n",
       "column            0.146456\n",
       "Name: 22, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the top features of an abstract from the dataset\n",
    "top_features = tfidf_df.iloc[22]\n",
    "top_features.sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding recommendations from tfidf and cosine similarity, using the sample abstract above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([230, 332, 757, 739,  14, 200], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the document similarities with sklearn linear_kernel.  Per sklearn,\n",
    "# linear_kernal is faster than cosine_similarity for tfidf.\n",
    "document_similarity = linear_kernel(article_df.iloc[article_idx:article_idx+1], tfidf_df).flatten()\n",
    "\n",
    "# Get the indices for the 5 documents that have highest cosine similarity to the sample.\n",
    "# If document_similarity == 1.0, then the abstract randomly selected matches an abstract in \n",
    "# the sample.  Abstracts were filtered for exact matches, so resubmisions with only a word\n",
    "# or two changed could cause this.  Throw that match out.\n",
    "related_indices = document_similarity.argsort()[:-7:-1]\n",
    "for i in range(0, len(related_indices)):\n",
    "    if related_indices[i] == 1.0:\n",
    "        related_indices.drop([i], inplace=True)\n",
    "related_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "      <th>terms</th>\n",
       "      <th>document_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>The gravitational waveform of a merging stella...</td>\n",
       "      <td>Higher order gravitational-wave modes with lik...</td>\n",
       "      <td>astro-ph.IM|astro-ph.HE|gr-qc</td>\n",
       "      <td>0.244862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>Wave dispersion in a pulsar plasma (a 1D, stro...</td>\n",
       "      <td>Wave dispersion in pulsar plasma: 1. Plasma re...</td>\n",
       "      <td>physics.plasm-ph|astro-ph.HE</td>\n",
       "      <td>0.226747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>The equipartition of magnetic and thermal ener...</td>\n",
       "      <td>Identification of magnetosonic modes in Galact...</td>\n",
       "      <td>physics.plasm-ph|astro-ph.GA</td>\n",
       "      <td>0.220757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>In the standard inflationary paradigm, cosmolo...</td>\n",
       "      <td>In search of an observational quantum signatur...</td>\n",
       "      <td>gr-qc|astro-ph.CO|quant-ph</td>\n",
       "      <td>0.181183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The fact that the spatial nonlocality of galax...</td>\n",
       "      <td>A new scale in the bias expansion</td>\n",
       "      <td>astro-ph.CO|astro-ph.GA</td>\n",
       "      <td>0.180749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>We analyse the Planck full-mission cosmic micr...</td>\n",
       "      <td>Planck 2018 results. IX. Constraints on primor...</td>\n",
       "      <td>astro-ph.CO|gr-qc|hep-ph|hep-th</td>\n",
       "      <td>0.174493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              abstract  \\\n",
       "230  The gravitational waveform of a merging stella...   \n",
       "332  Wave dispersion in a pulsar plasma (a 1D, stro...   \n",
       "757  The equipartition of magnetic and thermal ener...   \n",
       "739  In the standard inflationary paradigm, cosmolo...   \n",
       "14   The fact that the spatial nonlocality of galax...   \n",
       "200  We analyse the Planck full-mission cosmic micr...   \n",
       "\n",
       "                                                 title  \\\n",
       "230  Higher order gravitational-wave modes with lik...   \n",
       "332  Wave dispersion in pulsar plasma: 1. Plasma re...   \n",
       "757  Identification of magnetosonic modes in Galact...   \n",
       "739  In search of an observational quantum signatur...   \n",
       "14                   A new scale in the bias expansion   \n",
       "200  Planck 2018 results. IX. Constraints on primor...   \n",
       "\n",
       "                               terms  document_similarity  \n",
       "230    astro-ph.IM|astro-ph.HE|gr-qc             0.244862  \n",
       "332     physics.plasm-ph|astro-ph.HE             0.226747  \n",
       "757     physics.plasm-ph|astro-ph.GA             0.220757  \n",
       "739       gr-qc|astro-ph.CO|quant-ph             0.181183  \n",
       "14           astro-ph.CO|astro-ph.GA             0.180749  \n",
       "200  astro-ph.CO|gr-qc|hep-ph|hep-th             0.174493  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a df with the attributes of the 5 similar documents\n",
    "related_abstracts = df[['abstract', 'title', 'terms']].iloc[related_indices]\n",
    "related_abstracts['document_similarity'] = document_similarity[related_indices]\n",
    "\n",
    "# Print out the most similar documents\n",
    "related_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mode            0.377844\n",
       "wavelength      0.363548\n",
       "long            0.262896\n",
       "bias            0.257492\n",
       "modulation      0.223260\n",
       "local           0.221206\n",
       "relationship    0.208303\n",
       "correction      0.186903\n",
       "short           0.151912\n",
       "galaxy          0.149452\n",
       "Name: 112, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the features of the sample article\n",
    "sample_features = article_df.iloc[article_idx]\n",
    "sample_features.sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mode             0.504678\n",
       "high order       0.447306\n",
       "order            0.424577\n",
       "bayesian         0.185420\n",
       "high             0.160454\n",
       "inference        0.143651\n",
       "lead             0.135658\n",
       "binary           0.127623\n",
       "gravitational    0.118445\n",
       "method           0.116768\n",
       "Name: 230, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the features for the highest-ranked related article\n",
    "related_features = tfidf_df.iloc[related_indices[0]]\n",
    "related_features.sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The gravitational waveform of a merging stellar-mass binary is described at leading order by a quadrupolar mode. However, the complete waveform includes higher-order modes, which encode valuable information not accessible from the leading-order mode alone. Despite this, the majority of astrophysical inferences so far obtained with observations of gravitational waves employ only the leading order mode because calculations with higher-order modes are often computationally challenging. We show how to efficiently incorporate higher-order modes into astrophysical inference calculations with a two step procedure. First, we carry out Bayesian parameter estimation using a computationally cheap leading-order-mode waveform, which provides an initial estimate of binary parameters. Second, we weight the initial estimate using higher-order mode waveforms in order to fold in the extra information from the full waveform. We use mock data to demonstrate the effectiveness of this method. We apply the method to each binary black hole event in the first gravitational-wave transient catalog GWTC-1 to obtain posterior distributions and Bayesian evidence with higher-order modes. Performing Bayesian model selection on the events in GWTC-1, we find only a weak preference for waveforms with higher order modes. We discuss how this method can be generalized to a variety of other applications.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the abstract for the highest-ranked related article\n",
    "# Abstract is unaltered from download (more human-readable but includes formatting).\n",
    "df.abstract[related_indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Second model: gensim word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set up the filepaths for the modeling\n",
    "sentences_gensim = os.path.join('..', 'data', 'intermediate', 'sentences_gensim.txt')\n",
    "abstracts_gensim = os.path.join('..', 'data', 'intermediate', 'abstracts_gensim.txt')\n",
    "bi_sentences = os.path.join('..', 'data', 'intermediate', 'bi_sentences.txt')\n",
    "tri_sentences = os.path.join('..', 'data', 'intermediate', 'tri_sentences.txt')\n",
    "\n",
    "# Write the training sentences and the sample abstracts to a file so they can be streamed \n",
    "with open(sentences_gensim, 'w') as out_file:\n",
    "     for sent in text[:train_docs]:\n",
    "            out_file.write(sent + '\\n')\n",
    "            \n",
    "with open(abstracts_gensim, 'w') as out_file:\n",
    "     for sent in text[train_docs:]:\n",
    "            out_file.write(sent + '\\n')\n",
    "            \n",
    "# Set up streaming for the training data (create generator)\n",
    "unigram_sentences = LineSentence(sentences_gensim)\n",
    "bigram_sentences = LineSentence(bi_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up and run the phrasing modes that will create bi- and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Use Phrases to create bigram model from the unigram sentences\n",
    "biphrases = Phrases(unigram_sentences)\n",
    "bigram_model = Phraser(biphrases)\n",
    "\n",
    "# Write the bigram sentences\n",
    "with open(bi_sentences, 'w') as out_file:\n",
    "    for sent in unigram_sentences:\n",
    "        sent_out = ' '.join(bigram_model[sent])\n",
    "        out_file.write(sent_out + '\\n')\n",
    "\n",
    "# Use Phrases to create trigram model from the bigram sentences\n",
    "triphrases = Phrases(bigram_sentences)\n",
    "trigram_model = Phraser(triphrases)\n",
    "\n",
    "# Write the trigram sentences\n",
    "with open(tri_sentences, 'w') as out_file:\n",
    "    for sent in bigram_sentences:\n",
    "        sent_out = ' '.join(trigram_model[sent])\n",
    "        # Phrasing finished; remove stopwords here\n",
    "        out_file.write(remove_stopwords(sent_out) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77383, 80848)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the vocabulary sizes\n",
    "len(biphrases.vocab), len(triphrases.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black_hole',\n",
       " 'gravitational_wave',\n",
       " 'theory',\n",
       " 'gamma_ray',\n",
       " 'enrico',\n",
       " 'fermi',\n",
       " 'isaac',\n",
       " 'newton',\n",
       " 'fermi',\n",
       " 'gamma_ray',\n",
       " 'burst',\n",
       " 'monitor',\n",
       " 'ligo',\n",
       " 'interferometer',\n",
       " 'dark_matter',\n",
       " 'dark_energy']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing with random words that stand a chance of being in the phrases\n",
    "trigram_model[['black', 'hole', 'gravitational', 'wave', 'theory', 'gamma', 'ray', 'enrico', 'fermi', 'isaac', 'newton', \n",
    "              'fermi', 'gamma', 'ray', 'burst', 'monitor', 'ligo', 'interferometer', 'dark', 'matter', 'dark', 'energy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the word2vec model and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a staging directory for w2v\n",
    "w2v_model_staging = os.path.join('..', 'data', 'intermediate', 'w2v_model_stage')\n",
    "\n",
    "# Set up the generator for w2v\n",
    "w2v_sentences = LineSentence(tri_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initiate the model and train neural net\n",
    "# sg=0 is CBOW, sg=1 is skipgram\n",
    "abstracts2vec = Word2Vec(w2v_sentences, size=100, window=5, min_count=10, sg=1, workers=4, iter=20)\n",
    "abstracts2vec.save(w2v_model_staging)\n",
    "\n",
    "# Load the finished model\n",
    "abstracts2vec = Word2Vec.load(w2v_model_staging)\n",
    "abstracts2vec.init_sims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 epochs\n",
      "800 abstracts\n",
      "1744 words\n",
      "100 vectors\n"
     ]
    }
   ],
   "source": [
    "print(abstracts2vec.epochs, 'epochs')\n",
    "print(abstracts2vec.corpus_count, 'abstracts')\n",
    "print(abstracts2vec.wv.vectors.shape[0], 'words')\n",
    "print(abstracts2vec.wv.vectors.shape[1], 'vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('distance', 0.6301395893096924),\n",
       " ('sim_kpc', 0.5542659759521484),\n",
       " ('euv_wave', 0.5490813255310059),\n",
       " ('nearly', 0.5418782234191895),\n",
       " ('north', 0.540489912033081)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts2vec.wv.most_similar(positive='sun', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('electromagnetic_counterpart', 0.8779491186141968),\n",
       " ('advanced', 0.8622809648513794),\n",
       " ('virgo', 0.853947103023529),\n",
       " ('binary_neutron_star_merger', 0.8268074989318848),\n",
       " ('collaboration', 0.7956525087356567)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts2vec.wv.most_similar(positive='ligo', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dark_energy', 0.7869243621826172),\n",
       " ('dark_energy_model', 0.7838229537010193),\n",
       " ('modify_gravity', 0.7807337641716003),\n",
       " ('distinguish', 0.7380958795547485),\n",
       " ('cosmological_constant', 0.733410120010376)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts2vec.wv.most_similar(positive='cosmology', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('heating', 0.7054296135902405),\n",
       " ('inner_disk', 0.6626993417739868),\n",
       " ('keplerian', 0.6554714441299438),\n",
       " ('lifetime', 0.6469238996505737),\n",
       " ('angular_momentum', 0.6453890800476074)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts2vec.wv.most_similar(positive='rotational', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('life', 0.6789458990097046),\n",
       " ('comet', 0.6759127378463745),\n",
       " ('sized', 0.6626361608505249),\n",
       " ('solar_system', 0.657239556312561),\n",
       " ('terrestrial', 0.6457377672195435)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts2vec.wv.most_similar(positive=['sun', 'earth'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lat', 0.8250271081924438),\n",
       " ('electromagnetic_counterpart', 0.8219002485275269),\n",
       " ('advanced', 0.7894283533096313),\n",
       " ('binary_neutron_star_merger', 0.781348705291748),\n",
       " ('fast_radio', 0.7430184483528137)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts2vec.wv.most_similar(positive=['ligo', 'fermi'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('continue', 0.7618107795715332),\n",
       " ('electromagnetic_counterpart', 0.7318164110183716),\n",
       " ('virgo', 0.7140432000160217),\n",
       " ('observational_constraint', 0.7047683000564575),\n",
       " ('binary_neutron_star_merger', 0.7031246423721313)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts2vec.wv.most_similar(positive=['ligo', 'gamma'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('binary_neutron_star_merger', 0.8669383525848389),\n",
       " ('virgo', 0.8581392765045166),\n",
       " ('electromagnetic_counterpart', 0.8285713195800781),\n",
       " ('collaboration', 0.8218762874603271),\n",
       " ('advanced', 0.8203611373901367)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts2vec.wv.most_similar(positive=['ligo', 'counterpart'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('electromagnetic_counterpart', 0.7022133469581604),\n",
       " ('ligo', 0.6952662467956543),\n",
       " ('compact_object', 0.6912593841552734),\n",
       " ('binary_neutron_star_merger', 0.6666845679283142),\n",
       " ('compact_binary', 0.6530908346176147)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts2vec.wv.most_similar(positive=['binary', 'neutron'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

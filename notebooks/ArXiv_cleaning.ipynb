{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a pared-down normalization since I want to use spaCy to parse the text, and it requires\n",
    "# certain standard preprocessing steps NOT be performed (i.e., lowercasing words).\n",
    "\n",
    "def normalize(text):\n",
    "    '''Convert to ascii, remove special characters associated with LaTeX when given a df column'''\n",
    "    normalized_text = []\n",
    "    \n",
    "    for t in text:\n",
    "        t = unicodedata.normalize('NFKD', t).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        t = re.sub('\\\\\\\\', ' ', t)\n",
    "        t = re.sub('[${}]', '', t)\n",
    "        normalized_text.append(t)\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "# This function is to remove punctuation after spaCy parsing.\n",
    "\n",
    "def remove(token):\n",
    "    '''Provide feedback on whether a token is punctuation, whitespace, or stopword'''\n",
    "    return token.is_punct or token.is_space or token.is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up spaCy's English abilities\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\data\\\\astro_raw_137k'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = os.path.join('..', 'data', 'astro_raw_137k')\n",
    "\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the journal abstracts.  Second, drop the 'journal_ref' column since it means nothing (all values = 'No journal reference found'), then drop the 'id' and 'pdf_url' columns since 'url' contains the same information.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file, index_col=0, nrows=50000).reset_index(drop=True)\n",
    "df.drop(['journal_ref', 'id', 'pdf_url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 10 columns):\n",
      "abstract        50000 non-null object\n",
      "authors         50000 non-null object\n",
      "comment         50000 non-null object\n",
      "main_author     50000 non-null object\n",
      "publish_date    50000 non-null object\n",
      "term            50000 non-null object\n",
      "terms           50000 non-null object\n",
      "title           50000 non-null object\n",
      "update_date     50000 non-null object\n",
      "url             50000 non-null object\n",
      "dtypes: object(10)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the records and what we have.  \n",
    "\n",
    "First, unique abstracts.  Duplicates would be unexpected.  You wouldn't think that two abstracts would randomly be exactly the same.  You could, however, expect it if they were a reference to conference proceedings.  I'll look for that.  Hm...  It looks like there are cases might be explained by the same abstract being used for multiple conferences.  I'll drop the earlier version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49998"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.abstract.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.abstract) - df.abstract.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4094     The IMAGINE Consortium aims to bring modeling ...\n",
       "16388    Neutron stars are the endpoint of the life of ...\n",
       "24620    In this conference proceeding, I discuss in de...\n",
       "27709    Index of H.E.S.S. conference proceedings to th...\n",
       "34075    As the density of matter increases, atomic nuc...\n",
       "36813    It is understood that strong magnetic fields a...\n",
       "39994    The LUX collaboration new results advance the ...\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.abstract.loc[df.abstract.str.contains('conference proceeding')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>comment</th>\n",
       "      <th>main_author</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>term</th>\n",
       "      <th>terms</th>\n",
       "      <th>title</th>\n",
       "      <th>update_date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37026</th>\n",
       "      <td>The Telescope Array (TA) shows a 20$^{\\circ}$ ...</td>\n",
       "      <td>Noemie Globus, Denis Allard, Etienne Parizot, ...</td>\n",
       "      <td>28 pages, 19 figures, accepted in ApJ</td>\n",
       "      <td>Tsvi Piran</td>\n",
       "      <td>2016-10-17 20:00:16+00:00</td>\n",
       "      <td>astro-ph.HE</td>\n",
       "      <td>astro-ph.HE</td>\n",
       "      <td>Can we reconcile the TA excess and hotspot wit...</td>\n",
       "      <td>2017-01-26 23:48:38+00:00</td>\n",
       "      <td>http://arxiv.org/abs/1610.05319v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37507</th>\n",
       "      <td>ImageJ is a graphical user interface (GUI) dri...</td>\n",
       "      <td>Karen A. Collins, John F. Kielkopf, Keivan G. ...</td>\n",
       "      <td>Accepted by AJ</td>\n",
       "      <td>Frederic V. Hessman</td>\n",
       "      <td>2016-01-11 21:00:04+00:00</td>\n",
       "      <td>astro-ph.IM</td>\n",
       "      <td>astro-ph.IM|astro-ph.EP|astro-ph.SR</td>\n",
       "      <td>AstroImageJ: Image Processing and Photometric ...</td>\n",
       "      <td>2017-01-17 09:51:05+00:00</td>\n",
       "      <td>http://arxiv.org/abs/1601.02622v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract  \\\n",
       "37026  The Telescope Array (TA) shows a 20$^{\\circ}$ ...   \n",
       "37507  ImageJ is a graphical user interface (GUI) dri...   \n",
       "\n",
       "                                                 authors  \\\n",
       "37026  Noemie Globus, Denis Allard, Etienne Parizot, ...   \n",
       "37507  Karen A. Collins, John F. Kielkopf, Keivan G. ...   \n",
       "\n",
       "                                     comment          main_author  \\\n",
       "37026  28 pages, 19 figures, accepted in ApJ           Tsvi Piran   \n",
       "37507                         Accepted by AJ  Frederic V. Hessman   \n",
       "\n",
       "                    publish_date         term  \\\n",
       "37026  2016-10-17 20:00:16+00:00  astro-ph.HE   \n",
       "37507  2016-01-11 21:00:04+00:00  astro-ph.IM   \n",
       "\n",
       "                                     terms  \\\n",
       "37026                          astro-ph.HE   \n",
       "37507  astro-ph.IM|astro-ph.EP|astro-ph.SR   \n",
       "\n",
       "                                                   title  \\\n",
       "37026  Can we reconcile the TA excess and hotspot wit...   \n",
       "37507  AstroImageJ: Image Processing and Photometric ...   \n",
       "\n",
       "                     update_date                                url  \n",
       "37026  2017-01-26 23:48:38+00:00  http://arxiv.org/abs/1610.05319v2  \n",
       "37507  2017-01-17 09:51:05+00:00  http://arxiv.org/abs/1601.02622v2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dups = df.loc[df.duplicated('abstract')==True]\n",
    "dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.drop(dups.index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more check on the abstracts: how many of these papers have been withdrawn?  They need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>comment</th>\n",
       "      <th>main_author</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>term</th>\n",
       "      <th>terms</th>\n",
       "      <th>title</th>\n",
       "      <th>update_date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [abstract, authors, comment, main_author, publish_date, term, terms, title, update_date, url]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "withdrawn = df.loc[df.abstract.str.contains('withdrawn')]\n",
    "withdrawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(withdrawn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(withdrawn.index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, on to other columns.  I'll skip over columns like 'author' where uniqueness can't help us.\n",
    "\n",
    "Title duplicates are understandable, but they might need to be cleaned later.  I'll store them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_dups = df.loc[df.duplicated('title')==True]\n",
    "len(title_dups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure that only the 6 subcategories are included as the primary category since astro-ph was deprecated in 2009.  Although arXiv has gone back and reclassified some of the articles in the 'terms' column, it's missed some and put gibberish in for others.  Drop the articles with the primary category 'astro-ph' that appear in the df now.  This classification won't help us map back to the six categories later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.term.loc[df.term.str.startswith('ast')].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.loc[df.term=='astro-ph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "deprecated_prim = df.term.loc[df.term=='astro-ph'].index\n",
    "df.drop(deprecated_prim,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the info shows what we expect after dropping the excess columns, abstract duplicates, and withdrawn papers.  It also shows that there are no null values in the df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49973 entries, 0 to 49972\n",
      "Data columns (total 10 columns):\n",
      "abstract        49973 non-null object\n",
      "authors         49973 non-null object\n",
      "comment         49973 non-null object\n",
      "main_author     49973 non-null object\n",
      "publish_date    49973 non-null object\n",
      "term            49973 non-null object\n",
      "terms           49973 non-null object\n",
      "title           49973 non-null object\n",
      "update_date     49973 non-null object\n",
      "url             49973 non-null object\n",
      "dtypes: object(10)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.reset_index(drop=True,inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to clean everything up\n",
    "\n",
    "Steps:\n",
    "*  I won't try to strip html or keep LaTeX formatting.  These aren't real LaTeX documents that could be converted; they're snippets of LaTeX that represent equations and Greek letters.  The symbols could be useful in other circumstances but don't contribute value to this project.  \n",
    "*  I will use unicodedata.normalize to convert special characters and a simple regex to remove the LaTeX leftovers.\n",
    "*  I'll let spaCy lowercase, strip punctuation, and lemmatize the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = normalize(df.abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We simulate the evolution of a dust universe from z=1089 to z=0 by numerically integrating the Einstein's equation for a spatially flat Friedmann-Lemaire-Robertson-Walker (FLRW) background spacetime with scalar perturbations which are derived from the matter power spectrum produced with the Code for Anisotropies in the Microwave Background (CAMB). To investigate the effects of primordial gravitational waves (GWs) on the inhomogeneity of the universe, we add an additional decaying, divergenceless and traceless primordial tensor perturbation with its initial amplitude being 3 times 10^-4 to the above metric. We find that this primordial tensor perturbation suppresses the matter power spectrum by about 0.01 % at z=0 for modes with wave number similar to its. This suppression may be a possible probe of a GWs background in the future.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_abs = df.text[32]\n",
    "sample_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_abs = nlp(sample_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>part of speech</th>\n",
       "      <th>entity type</th>\n",
       "      <th>stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simulate</td>\n",
       "      <td>simulate</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>evolution</td>\n",
       "      <td>evolution</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dust</td>\n",
       "      <td>dust</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>universe</td>\n",
       "      <td>universe</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>from</td>\n",
       "      <td>from</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>z=1089</td>\n",
       "      <td>z=1089</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>z=0</td>\n",
       "      <td>z=0</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>by</td>\n",
       "      <td>by</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>numerically</td>\n",
       "      <td>numerically</td>\n",
       "      <td>ADV</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>integrating</td>\n",
       "      <td>integrate</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Einstein</td>\n",
       "      <td>einstein</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>ORG</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>'s</td>\n",
       "      <td>'s</td>\n",
       "      <td>PART</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>equation</td>\n",
       "      <td>equation</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>spatially</td>\n",
       "      <td>spatially</td>\n",
       "      <td>ADV</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>flat</td>\n",
       "      <td>flat</td>\n",
       "      <td>ADJ</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Friedmann</td>\n",
       "      <td>friedmann</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Lemaire</td>\n",
       "      <td>lemaire</td>\n",
       "      <td>PROPN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Robertson</td>\n",
       "      <td>robertson</td>\n",
       "      <td>PROPN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Walker</td>\n",
       "      <td>walker</td>\n",
       "      <td>PROPN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>by</td>\n",
       "      <td>by</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>about</td>\n",
       "      <td>about</td>\n",
       "      <td>ADV</td>\n",
       "      <td>PERCENT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NUM</td>\n",
       "      <td>PERCENT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>%</td>\n",
       "      <td>%</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>PERCENT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>at</td>\n",
       "      <td>at</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>z=0</td>\n",
       "      <td>z=0</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>modes</td>\n",
       "      <td>mode</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>wave</td>\n",
       "      <td>wave</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>number</td>\n",
       "      <td>number</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>similar</td>\n",
       "      <td>similar</td>\n",
       "      <td>ADJ</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>its</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>ADJ</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>This</td>\n",
       "      <td>this</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>suppression</td>\n",
       "      <td>suppression</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>may</td>\n",
       "      <td>may</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>possible</td>\n",
       "      <td>possible</td>\n",
       "      <td>ADJ</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>probe</td>\n",
       "      <td>probe</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>GWs</td>\n",
       "      <td>gws</td>\n",
       "      <td>ADJ</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>background</td>\n",
       "      <td>background</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>future</td>\n",
       "      <td>future</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           token        lemma part of speech entity type  stopword\n",
       "0             We       -PRON-           PRON                 False\n",
       "1       simulate     simulate           VERB                 False\n",
       "2            the          the            DET                  True\n",
       "3      evolution    evolution           NOUN                 False\n",
       "4             of           of            ADP                  True\n",
       "5              a            a            DET                  True\n",
       "6           dust         dust           NOUN                 False\n",
       "7       universe     universe           NOUN                 False\n",
       "8           from         from            ADP                  True\n",
       "9         z=1089       z=1089           NOUN                 False\n",
       "10            to           to            ADP                  True\n",
       "11           z=0          z=0           NOUN                 False\n",
       "12            by           by            ADP                  True\n",
       "13   numerically  numerically            ADV                 False\n",
       "14   integrating    integrate           VERB                 False\n",
       "15           the          the            DET                  True\n",
       "16      Einstein     einstein          PROPN         ORG     False\n",
       "17            's           's           PART                 False\n",
       "18      equation     equation           NOUN                 False\n",
       "19           for          for            ADP                  True\n",
       "20             a            a            DET                  True\n",
       "21     spatially    spatially            ADV                 False\n",
       "22          flat         flat            ADJ                 False\n",
       "23     Friedmann    friedmann          PROPN      PERSON     False\n",
       "24             -            -          PUNCT                 False\n",
       "25       Lemaire      lemaire          PROPN                 False\n",
       "26             -            -          PUNCT                 False\n",
       "27     Robertson    robertson          PROPN                 False\n",
       "28             -            -          PUNCT                 False\n",
       "29        Walker       walker          PROPN                 False\n",
       "..           ...          ...            ...         ...       ...\n",
       "117           by           by            ADP                  True\n",
       "118        about        about            ADV     PERCENT      True\n",
       "119         0.01         0.01            NUM     PERCENT     False\n",
       "120            %            %           NOUN     PERCENT     False\n",
       "121           at           at            ADP                  True\n",
       "122          z=0          z=0           NOUN    CARDINAL     False\n",
       "123          for          for            ADP                  True\n",
       "124        modes         mode           NOUN                 False\n",
       "125         with         with            ADP                  True\n",
       "126         wave         wave           NOUN                 False\n",
       "127       number       number           NOUN                 False\n",
       "128      similar      similar            ADJ                 False\n",
       "129           to           to            ADP                  True\n",
       "130          its       -PRON-            ADJ                  True\n",
       "131            .            .          PUNCT                 False\n",
       "132         This         this            DET                 False\n",
       "133  suppression  suppression           NOUN                 False\n",
       "134          may          may           VERB                  True\n",
       "135           be           be           VERB                  True\n",
       "136            a            a            DET                  True\n",
       "137     possible     possible            ADJ                 False\n",
       "138        probe        probe           NOUN                 False\n",
       "139           of           of            ADP                  True\n",
       "140            a            a            DET                  True\n",
       "141          GWs          gws            ADJ                 False\n",
       "142   background   background           NOUN                 False\n",
       "143           in           in            ADP                  True\n",
       "144          the          the            DET                  True\n",
       "145       future       future           NOUN                 False\n",
       "146            .            .          PUNCT                 False\n",
       "\n",
       "[147 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = [token.text for token in parsed_abs]\n",
    "token_lem = [token.lemma_ for token in parsed_abs]\n",
    "token_speech = [token.pos_ for token in parsed_abs]\n",
    "token_ent = [token.ent_type_ for token in parsed_abs]\n",
    "token_stop = [token.is_stop for token in parsed_abs]\n",
    "pd.DataFrame(list(zip(token_text, token_lem, token_speech, token_ent, token_stop)), \n",
    "             columns=['token','lemma','part of speech','entity type','stopword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(parsed_abs.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "we simulate evolution dust universe z=1089 z=0 numerically integrate einstein 's equation spatially flat friedmann lemaire robertson walker flrw background spacetime scalar perturbation derive matter power spectrum produce code anisotropies microwave background camb\n",
      "\n",
      "to investigate effect primordial gravitational wave gws inhomogeneity universe add additional decaying divergenceless traceless primordial tensor perturbation initial amplitude 3 time 10 ^ -4 metric\n",
      "\n",
      "we find primordial tensor perturbation suppress matter power spectrum 0.01 z=0 mode wave number similar\n",
      "\n",
      "this suppression possible probe gws background future\n"
     ]
    }
   ],
   "source": [
    "for sent in parsed_abs.sents:\n",
    "    print('')\n",
    "    print(' '.join([token.lemma_ if token.lemma_ != '-PRON-' else token.text.lower()\n",
    "                    for token in sent if not remove(token)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def pipeline(text)\n",
    "\n",
    "#test_text = df.text[:10]\n",
    "\n",
    "sentences = []\n",
    "new_text = []\n",
    "\n",
    "for doc in nlp.pipe(df.text, batch_size=50):\n",
    "    assert doc.is_parsed\n",
    "    sentences.append(len(list(doc.sents)))\n",
    "    stmt = ' '.join([token.lemma_ if token.lemma_ != '-PRON-' else token.text.lower()\n",
    "                     for token in doc if not remove(token)])\n",
    "    new_text.append(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49973"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49973"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentences'] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join('..','data','astro_intermediate.pickle')\n",
    "\n",
    "with open(file, 'wb') as f:\n",
    "    pickle.dump(df, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.meta['lang'])\n",
    "print(nlp.meta['pipeline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join('..','data','astro_intermediate.csv')\n",
    "\n",
    "df.to_csv(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
